{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "caded33afa80e27ddd392cf31936d682",
     "grade": false,
     "grade_id": "cell-11ef9fbc6790a68b",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Regresion Lineal por el metodo de Ordinary Least Squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9693a553f0b18436ffb9975dee33e728",
     "grade": false,
     "grade_id": "cell-81a16b42de1bb106",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Los modelos predictivos de Scikit-learn se definen como clases que tienen al menos cuatro métodos:\n",
    "* ``__init__``: el constructor, recibe como argumentos los meta-parámetros del modelo y los almacena\n",
    "* ``fit``: recibe como argumentos X e y, entrena el modelo (calcula sus parámetros) y lo devuelve.\n",
    "* ``predict``: recibe como argumento X, calcula las predicciones y las devuelve.\n",
    "* ``score``: recibe como argumentos X e y, realiza las predicciones sobre X y calcula con ellas el error de aproximación a y. Por defecto aplica la métrica ``r2_score`` a los modelos de regresión y la métrica ``accuracy`` a los de clasifiación.\n",
    "\n",
    "Programa a continuación los métodos fit y predict de la regresión lineal mediante el método de Ordinary Least Squares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "de251aef75455bf4d179638a0200063f",
     "grade": false,
     "grade_id": "cell-c40d4400f0cdd222",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "class OLSLinearRegression(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if len(X.shape) == 1: X = X.reshape((-1, 1))\n",
    "        X = np.hstack((np.ones((len(X), 1)), X))\n",
    "        \n",
    "        # In this case, @ is for matrix multiplication\n",
    "        self.beta_ = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "        return self\n",
    "        \n",
    "    def predict(self, X):\n",
    "        # YOUR CODE HERE\n",
    "        \n",
    "        if len(X.shape) == 1: X = X.reshape((-1, 1))\n",
    "        X = np.hstack((np.ones((len(X), 1)), X))\n",
    "        \n",
    "        return X @ self.beta_\n",
    "    \n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return r2_score(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0d833e0e183bdfddcd2b1cb88988d975",
     "grade": false,
     "grade_id": "cell-dcc5eef402a0a268",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "La regresión lineal con el método de OLS ya está disponible en Scikit-learn mediante la clase ``LinearRegression`` del submódulo ``linear_model``. Si has programado el método de OLS correctamente, debería producir los mismos resultados que ``LinearRegression`` sobre los datasets de regresión de Boston Housing y Diabetes, que se obtienen con las funciones de Scikit-learn ``load_boston`` y ``load_diabetes``, respectivamente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ab483a58b5e88d18a26e37e1bc40f41f",
     "grade": true,
     "grade_id": "cell-de1ef339d923b5a4",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_boston:\n",
      "\tR2 OLS: 0.6354638433201965\n",
      "\tR2 LS: 0.6354638433202128\n",
      "load_diabetes:\n",
      "\tR2 OLS: 0.35940090989715434\n",
      "\tR2 LS: 0.35940090989715545\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "for loader in (load_boston, load_diabetes):\n",
    "    X, y = loader(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    ols_lr = OLSLinearRegression()\n",
    "    lr = LinearRegression()\n",
    "\n",
    "    ols_lr.fit(X_train, y_train)\n",
    "    lr.fit(X_train, y_train)\n",
    "\n",
    "    ols_lr_preds = ols_lr.predict(X_test)\n",
    "    lr_preds = lr.predict(X_test)\n",
    "    np.testing.assert_array_almost_equal(ols_lr_preds, lr_preds, decimal=3, err_msg='Las predicciones difieren demasiado')\n",
    "\n",
    "    ols_lr_score = ols_lr.score(X_test, y_test)\n",
    "    lr_score = lr.score(X_test, y_test)\n",
    "    print(loader.__name__ + ':\\n\\tR2 OLS: ' + str(ols_lr_score) + '\\n\\tR2 LS: ' + str(lr_score))\n",
    "    np.testing.assert_almost_equal(ols_lr_score, lr_score, decimal=2, err_msg='Los scores difieren demasiado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "e41991d49bd04fc501fbb2ce8f18b71d",
     "grade": false,
     "grade_id": "cell-030361b0cc95b0c0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Regresion Lineal por el metodo de Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "03e88b3d1f9b67d8f7574d47755b02dc",
     "grade": false,
     "grade_id": "cell-6961e2703cad8fca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "En esta ocasión debes implementar los métodos ``fit`` y ``predict`` de la regresión lineal mediante el método de descenso por gradiente estocástico:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "7e5370000b89c24fad676b754fa47813",
     "grade": false,
     "grade_id": "cell-070c6b1570479dc6",
     "locked": false,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.base import BaseEstimator, RegressorMixin\n",
    "from sklearn.metrics import r2_score\n",
    "from numpy import random\n",
    "\n",
    "class SGDLinearRegression(BaseEstimator, RegressorMixin):\n",
    "\n",
    "    def __init__(self, max_iter=1000, eta=0.01):\n",
    "        self.max_iter = max_iter\n",
    "        self.eta = eta\n",
    "\n",
    "    def initialize(self, X):\n",
    "        if len(X.shape) == 1: X = X.reshape((-1, 1))\n",
    "        X = np.hstack((np.ones((len(X), 1)), X))       \n",
    "        return X\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        X = self.initialize(X)  \n",
    "        \n",
    "        iter_ = self.max_iter\n",
    "        if( X.shape[0] < self.max_iter ):\n",
    "            iter_ = X.shape[0]\n",
    "        \n",
    "        # Random Order\n",
    "        index = random.choice(iter_,iter_,replace=False)\n",
    "        X = X[index,:]\n",
    "        y = y[index]\n",
    "        \n",
    "        # In this case, @ is for matrix multiplication\n",
    "        betha = np.linalg.inv(X.T @ X) @ X.T @ y \n",
    "        gradient = self.gradient(X, y, betha, iter_)\n",
    "        \n",
    "        self.betha = betha - self.eta * gradient \n",
    "        return self\n",
    "        \n",
    "    def gradient(self, X, y, betha, iter_):\n",
    "        gradient = 0\n",
    "        \n",
    "        for reg in range(iter_):\n",
    "            gradient = ( -2 /iter_) * X[reg] * ( y[reg] - ( X[reg] * betha ) )\n",
    "                   \n",
    "#         gradient *= (-2/iter_)\n",
    "#         return gradient\n",
    "        \n",
    "        return gradient\n",
    "        \n",
    "    \n",
    "    \n",
    "    def predict(self, X):\n",
    "        X = self.initialize(X)  \n",
    "        return X @ self.betha\n",
    "\n",
    "    def score(self, X, y):\n",
    "        preds = self.predict(X)\n",
    "        return r2_score(y, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "1dfe18a5d7d42972b7457bbfbf196efa",
     "grade": false,
     "grade_id": "cell-c0dd66c08a4fb10f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Se puede comprobar si los resultados son similares a los obtenidos con el modelo ``SGDRegressor`` de Scikit-learn. En este caso hay que tomar la precaución de estandarizar los datos de entrada, ya que los métodos basados en descenso por gradiente son sensibles a la escala de las variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "5b0de87f052f65522ea4b215c2751b5d",
     "grade": true,
     "grade_id": "cell-19548a2262e25af5",
     "locked": true,
     "points": 2,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load_boston:\n",
      "\tR2 SGDLR: 0.6351656112624102\n",
      "\tR2 SGDR: 0.6324450476096304\n",
      "load_diabetes:\n",
      "\tR2 SGDLR: 0.3594169323684281\n",
      "\tR2 SGDR: 0.32315572923945013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\nicole guzmán cleto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n",
      "c:\\users\\nicole guzmán cleto\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\sklearn\\linear_model\\stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston, load_diabetes\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "for loader in (load_boston, load_diabetes):\n",
    "    X, y = loader(return_X_y=True)\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=0)\n",
    "\n",
    "    sgd_lr = Pipeline([('stds', StandardScaler()), ('sgd_lr', SGDLinearRegression())])\n",
    "    sgd = Pipeline([('stds', StandardScaler()), ('sgd', SGDRegressor(penalty=None, shuffle=False, learning_rate='constant'))])\n",
    "\n",
    "    sgd_lr.fit(X_train, y_train)\n",
    "    sgd.fit(X_train, y_train)\n",
    "\n",
    "    sgd_lr_score = sgd_lr.score(X_test, y_test)\n",
    "    sgd_score = sgd.score(X_test, y_test)\n",
    "    print(loader.__name__ + ':\\n\\tR2 SGDLR: ' + str(sgd_lr_score) + '\\n\\tR2 SGDR: ' + str(sgd_score))\n",
    "    np.testing.assert_almost_equal(sgd_lr_score, sgd_score, decimal=1, err_msg='Los scores difieren demasiado')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0626d308ab32bce16f78bf052e3e9ec0",
     "grade": false,
     "grade_id": "cell-c9491d770b813f0a",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Boston Housing con redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8ddcf6baa00c83029c7d54d660e20340",
     "grade": false,
     "grade_id": "cell-59b0f3d543b9c3e7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Una vez entendidos los modelos lineales, estamos en condiciones de probar otros modelos más expresivos como las redes neuronales. Vamos a comprobar su efectividad con un conjunto de datos clásico en Aprendizaje Automático: Boston Housing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "be58f5d178a3c86a27189f303b230437",
     "grade": false,
     "grade_id": "cell-5478d66d95ac46fa",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "El dataset está disponible en la página web de UCI (https://archive.ics.uci.edu/ml/datasets/Housing), donde, aparte de descargarlo, podéis ver el tipo de variables que contiene, que es un dataset de regresión, y algunos detalles más. Por suerte para nosotros, scikit-learn ya dispone de una función que carga los datos de este dataset en nuestro espacio de trabajo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "0207a56819185fbb90e657f4cc48dc22",
     "grade": false,
     "grade_id": "cell-c879c4cbd4abc5cd",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13)\n",
      "[6.320e-03 1.800e+01 2.310e+00 0.000e+00 5.380e-01 6.575e+00 6.520e+01\n",
      " 4.090e+00 1.000e+00 2.960e+02 1.530e+01 3.969e+02 4.980e+00]\n",
      "24.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_boston\n",
    "boston = load_boston()\n",
    "print(boston.data.shape)\n",
    "print(str(boston.data[0]))\n",
    "print(str(boston.target[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "fa99549157be64c34f9f501505ca3531",
     "grade": false,
     "grade_id": "cell-6d3dabef794e1bc7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Para poder evaluar de forma fiable los modelos, lo primero que deberíamos hacer es definir folds para cross-validation o separar una parte de los datos como conjunto de test, por ejemplo un 10% de la muestra:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b4b0e4ca93bc8f178befa0c9f3d4c3ff",
     "grade": false,
     "grade_id": "cell-e826877ea7697abe",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(455, 13)\n",
      "(455,)\n",
      "(51, 13)\n",
      "(51,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(boston.data, boston.target, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9e1e6eb2d90aceeca38df35841939dd1",
     "grade": false,
     "grade_id": "cell-813a9c7253764fc9",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "A continuación, vamos a crear el modelo base contra el que comparar nuestras redes neuronales. Dado que el problema es de regresión, lo más básico que podemos utilizar es una regresión lineal. Usaremos la ya vista anteriormente, que se entrena mediante descenso por gradiente. Estos modelos, así como los de redes neuronales, necesitan una estandarización previa de los datos, por lo que vamos a construir un pipeline de procesamiento en el que la primera componente será un estandarizador y la segunda el modelo predictivo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "9cbba028f97127572a4692de27b2359c",
     "grade": false,
     "grade_id": "cell-96a87eed3b9abe16",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "estimator = Pipeline([('standardizer', StandardScaler()), ('predictor', SGDRegressor())])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d52f0af341282ecf1f1311e75643bd8e",
     "grade": false,
     "grade_id": "cell-26617082808022e8",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Ahora podemos entrenar nuestro modelo con el conjunto de datos de entrenamiento y evaluarlo con el conjunto de datos de test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "b6df4dc782556477200f461b66aa2dad",
     "grade": false,
     "grade_id": "cell-d1f38d9db2aaa61f",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6360770881587914\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/linear_model/stochastic_gradient.py:166: FutureWarning: max_iter and tol parameters have been added in SGDRegressor in 0.19. If both are left unset, they default to max_iter=5 and tol=None. If tol is not None, max_iter defaults to max_iter=1000. From 0.21, default max_iter will be 1000, and default tol will be 1e-3.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "estimator.fit(X_train, y_train)\n",
    "print(estimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c030d65c7184443bcf2b8f4e448e9e71",
     "grade": false,
     "grade_id": "cell-1ca9832d0ef4fbca",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "El resultado es mejorable, pero ya tenemos nuestro modelo base contra el que comparar. ¿Serías capaz de programar el mismo pipeline utilizando un perceptrón en lugar de una regresión lineal? La clase de scikit-learn que debes usar es sklearn.neural_network.MLPRegressor. Ten en cuenta que el número y tamaño de las capas ocultas se especifica con el meta-parámetro hidden_layer_sizes (por ejemplo, hidden_layer_sizes=() para un perceptrón simple y hidden_layer_sizes=(100, 100) para un perceptrón con dos capas ocultas de tamaño 100). Es probable que el modelo necesite muchas iteraciones de entrenamiento para converger. Puedes indicar el número máximo de iteraciones con el meta-parámetro max_iter (por ejemplo, max_iter=1000)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "61cc44617b8c7434093f8cd7a65ec46a",
     "grade": true,
     "grade_id": "cell-c384022c33a522bb",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8441277387391541\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mLPestimator = Pipeline([('standardizer', StandardScaler()), \n",
    "                         ('predictor', MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000))\n",
    "                        ])\n",
    "\n",
    "\n",
    "mLPestimator.fit(X_train, y_train)\n",
    "print(mLPestimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a47cbcbc7211f4ca437dfd751f5b4f2d",
     "grade": false,
     "grade_id": "cell-72b6dec298ac3748",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Hay modelos que pueden mejorarse a través de términos de regularización. La regresión lineal se puede regularizar con un término L2, lo que nos da el modelo de regresión ridge. Las redes neuronales son los modelos más flexibles en este sentido. Vamos a ver cómo funciona un modelo ridge. Debemos especificar el coeficiente de regularización en el meta-parámetro alpha. Los valores razonables suelen estar entre 0 y 1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d3bd99d7d87eb52b3abb8c02a13735a5",
     "grade": false,
     "grade_id": "cell-dffcb882861a4db7",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6350819555034828\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "estimator = Pipeline([('standardizer', StandardScaler()), ('predictor', Ridge(alpha=0.5))])\n",
    "estimator.fit(X_train, y_train)\n",
    "print(estimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "d70b3c8d6a560a18a8de31e6e6f0d073",
     "grade": false,
     "grade_id": "cell-83feef17b06bc8e6",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "El modelo MLPRegressor también tiene un parámetro alpha. ¿Puedes probar el modelo añadiendo regularización?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "6541e8f6d59c5e9469d04996df6d1fa8",
     "grade": true,
     "grade_id": "cell-c58ba1d38348c060",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8754142726690957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "mLPestimator = Pipeline([('standardizer', StandardScaler()), \n",
    "                         ('predictor', MLPRegressor(hidden_layer_sizes=(100, 100), max_iter=1000, alpha=0.5))\n",
    "                        ])\n",
    "\n",
    "\n",
    "mLPestimator.fit(X_train, y_train)\n",
    "print(mLPestimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "ebd4de3ff2b05e8b9049d5c0cfa57974",
     "grade": false,
     "grade_id": "cell-db3bdbab249d5b17",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Si has elegido bien el valor del coeficiente alpha, habrás visto una ligera mejora en tu modelo. Normalmente la búsqueda de meta-parámetros se automatiza con métodos como la búsqueda en rejilla o la búsqueda aleatoria. Vamos a probar la búsqueda en rejilla con el modelo ridge para su parámetro alpha:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "7d017799e4a6387cd793e7da56860d40",
     "grade": false,
     "grade_id": "cell-3e9f5958efc63632",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor__alpha': 1.0}\n",
      "0.6352832859647826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "estimator = Pipeline([('standardizer', StandardScaler()), ('predictor', Ridge())])\n",
    "search = GridSearchCV(estimator, {'predictor__alpha': [0.0, 0.25, 0.5, 0.75, 1.0]})\n",
    "search.fit(X_train, y_train)\n",
    "print(search.best_params_)\n",
    "print(search.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "2d2adac81b25fd862ff9100e52c96205",
     "grade": false,
     "grade_id": "cell-86f055ce407acfef",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "¿Puedes hacer lo mismo con el parámetro alpha del perceptrón multicapa? Puedes probar más valores aparte de 0, 0.25, 0.5, 0.75 y 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "51b28b6140c4f65703138b513a7e7999",
     "grade": true,
     "grade_id": "cell-00ee122f1e9801df",
     "locked": false,
     "points": 1,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2053: FutureWarning: You should specify a value for 'cv' instead of relying on the default value. The default value will change from 3 to 5 in version 0.22.\n",
      "  warnings.warn(CV_WARNING, FutureWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n",
      "/opt/conda/lib/python3.6/site-packages/sklearn/neural_network/multilayer_perceptron.py:562: ConvergenceWarning: Stochastic Optimizer: Maximum iterations (1000) reached and the optimization hasn't converged yet.\n",
      "  % self.max_iter, ConvergenceWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'predictor__alpha': 1, 'predictor__hidden_layer_sizes': (100, 100), 'predictor__max_iter': 1000}\n",
      "0.8558202915656691\n"
     ]
    }
   ],
   "source": [
    "# YOUR CODE HERE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe_params = {'predictor__hidden_layer_sizes': [(100, 100)],\n",
    "               'predictor__max_iter': [1000],\n",
    "               'predictor__alpha': [0.0, 0.25, 0.5, 0.75, 1]\n",
    "              }\n",
    "\n",
    "\n",
    "pipe = Pipeline([\n",
    "                 ('standardizer', StandardScaler()),\n",
    "                 ('predictor', MLPRegressor())\n",
    "                ])\n",
    "\n",
    "grid_search_cv = GridSearchCV(pipe, pipe_params)\n",
    "\n",
    "grid_search_cv.fit(X_train, y_train)\n",
    "print(grid_search_cv.best_params_)\n",
    "print(grid_search_cv.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c801295242f092f5e14c336f3c0b1696",
     "grade": false,
     "grade_id": "cell-560735fa0ae8f956",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "# Digits con redes neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "4087c4150b20fb5ae78190de7336c450",
     "grade": false,
     "grade_id": "cell-78a74afa439eaaf0",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Ahora vamos a experimentar con las redes neuronales en el problema Digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c9013b18a48c429174085924c7afc83c",
     "grade": false,
     "grade_id": "cell-42b991d6486c4c5c",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "La información sobre el dataset se puede encontrar en http://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_digits.html. Contiene imágenes de 8x8 píxeles de dígitos manuscritos, y el objetivo es clasificarlos correctamente dentro de las categorías de 0 a 9. Podemos descargarlo directamente utilizando scikit-learn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "3b0899996be103b127a0fc8314e4baa8",
     "grade": false,
     "grade_id": "cell-1c20f05319e3e7ec",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1797, 64)\n",
      "[ 0.  0.  5. 13.  9.  1.  0.  0.  0.  0. 13. 15. 10. 15.  5.  0.  0.  3.\n",
      " 15.  2.  0. 11.  8.  0.  0.  4. 12.  0.  0.  8.  8.  0.  0.  5.  8.  0.\n",
      "  0.  9.  8.  0.  0.  4. 11.  0.  1. 12.  7.  0.  0.  2. 14.  5. 10. 12.\n",
      "  0.  0.  0.  0.  6. 13. 10.  0.  0.  0.]\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_digits\n",
    "digits = load_digits()\n",
    "print(digits.data.shape)\n",
    "print(str(digits.data[0]))\n",
    "print(str(digits.target[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "876e09c701443c4a5396a9e62c6676ec",
     "grade": false,
     "grade_id": "cell-c15b2db595118622",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Podemos ver qué tipo de imágenes hay en el dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "8f8ff1c11d15e8196e6b2a6138af1583",
     "grade": false,
     "grade_id": "cell-523bd6ae1120d5d3",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 10 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "num_images = 10\n",
    "for index, image in enumerate(digits.images[:num_images]):\n",
    "    plt.subplot(2, num_images, index + num_images + 1)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image, cmap=plt.cm.gray_r, interpolation='nearest')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "649c9694efa500414058703c4aac0c70",
     "grade": false,
     "grade_id": "cell-8f0ca4a615a0be21",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Como siempre, lo correcto es hacer folds para cross-validation o al menos separar algunos patrones para test:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "24c0e116cfdcc49a819d0c621d07de28",
     "grade": false,
     "grade_id": "cell-76d5a41e3802cb86",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1617, 64)\n",
      "(1617,)\n",
      "(180, 64)\n",
      "(180,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=0.1)\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "f65c1a6c4b13c9b350134248339f26f2",
     "grade": false,
     "grade_id": "cell-7e906fee16e875cc",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Vamos a probar qué tal resuelve este problema de clasificación un perceptrón simple con alpha=0.1 (http://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "c47d304105fa0ed1023d639987be9fff",
     "grade": false,
     "grade_id": "cell-a4f11fca4fade282",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9555555555555556\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "estimator = Pipeline([('standardizer', StandardScaler()), ('predictor', MLPClassifier(hidden_layer_sizes=(), alpha=0.1, max_iter=10000))])\n",
    "estimator.fit(X_train, y_train)\n",
    "print(estimator.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "08d78306b6f116b2ccba1d0cf78aaa66",
     "grade": false,
     "grade_id": "cell-e4357ab70931907d",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "Scikit-learn está bastante limitado a la hora de experimentar con redes neuronales, ya que no permite crear redes de tipo convolucional o recurrente. Este problema tiene claramente una estructura espacial (los píxeles de cada imagen están colocados de una determinada forma, y su orden es importante), por lo que probablemente una red convolucional sea capaz de mejorar los resultados. Para probar este tipo de modelos, podemos utilizar la librería Keras (https://keras.io/):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "a0e5554170b50f601868405fb326ce10",
     "grade": false,
     "grade_id": "cell-913e4d0caf9d9b4e",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Test loss: 0.3524875437169764\n",
      "Test accuracy: 0.9611111111111111\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Flatten, Conv2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "input_shape = (1, 8, 8)\n",
    "X_train_keras = X_train.reshape(X_train.shape[0], *input_shape)\n",
    "X_test_keras = X_test.reshape(X_test.shape[0], *input_shape)\n",
    "X_train_keras = X_train_keras.astype('float32')\n",
    "X_test_keras = X_test_keras.astype('float32')\n",
    "X_train_keras /= 16\n",
    "X_test_keras /= 16\n",
    "num_classes = 10\n",
    "y_train_keras = to_categorical(y_train, num_classes)\n",
    "y_test_keras = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(1, 8, 8)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.fit(X_train_keras, y_train_keras, batch_size=32, epochs=1000, verbose=0, validation_split=0.1)\n",
    "score = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "checksum": "97f81fafed9a479b204e1aa7b4a1cf3a",
     "grade": false,
     "grade_id": "cell-89e31d52eda72466",
     "locked": true,
     "schema_version": 1,
     "solution": false
    }
   },
   "source": [
    "La red convolucional que acabamos de construir tiene únicamente una capa convolucional y una capa \"normal\" (fully-connected, densa). Podemos mejorar los resultados modificándola para que tenga dos capas convolucionales seguidas de dos capas densas. ¿Puedes hacerlo? Ten en cuenta que hay que controlar el tamaño de los kernels de las convoluciones (en este caso usamos (2, 2). Puedes ver documentación al respecto en http://deeplearning.net/tutorial/lenet.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.039126358015669715\n",
      "Test accuracy: 0.9944444444444445\n"
     ]
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.utils import to_categorical\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "K.set_image_data_format('channels_first')\n",
    "\n",
    "input_shape = (1, 8, 8)\n",
    "X_train_keras = X_train.reshape(X_train.shape[0], *input_shape)\n",
    "X_test_keras = X_test.reshape(X_test.shape[0], *input_shape)\n",
    "X_train_keras = X_train_keras.astype('float32')\n",
    "X_test_keras = X_test_keras.astype('float32')\n",
    "X_train_keras /= 16\n",
    "X_test_keras /= 16\n",
    "num_classes = 10\n",
    "y_train_keras = to_categorical(y_train, num_classes)\n",
    "y_test_keras = to_categorical(y_test, num_classes)\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "# Construct the first convolutional pooling layer:\n",
    "# maxpooling reduces this to (8/2, 8/2) = (4, 4)\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu', input_shape=(1, 8, 8)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Conv2D(32, kernel_size=(2, 2), activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes, activation='softmax'))\n",
    "\n",
    "model.compile(loss=categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])\n",
    "model.fit(X_train_keras, y_train_keras, batch_size=32, epochs=1000, verbose=0, validation_split=0.1)\n",
    "score = model.evaluate(X_test_keras, y_test_keras, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "checksum": "d608309fc170429036c3c0bbb76a68a0",
     "grade": true,
     "grade_id": "cell-1220ab486f8a68d9",
     "locked": false,
     "points": 2,
     "schema_version": 1,
     "solution": true
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
